{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RMbWEWf1-DhP"
   },
   "source": [
    "# System cmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Byw8AilW-IPG"
   },
   "outputs": [],
   "source": [
    "!kill -9 -1 # Kill current runtime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3YVYE7dTwki0"
   },
   "outputs": [],
   "source": [
    "!python3 -c 'import tensorflow as tf; print(tf.__version__)'  # Print tensorflow version: 1.10.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t9SjjzXOpIHD"
   },
   "outputs": [],
   "source": [
    "!cat /etc/*release  # Check ubuntu version: 18.04"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_xkA5jm9l__J"
   },
   "source": [
    "# Link folder to google drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1002
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 12161,
     "status": "ok",
     "timestamp": 1541314641023,
     "user": {
      "displayName": "Hoang Thanh",
      "photoUrl": "",
      "userId": "16863023710723783343"
     },
     "user_tz": -540
    },
    "id": "FTeqzzIEAHLX",
    "outputId": "005db570-904c-4817-e245-7d894fb3810c"
   },
   "outputs": [],
   "source": [
    "# Check cuda version\n",
    "!nvidia-smi\n",
    "!ldconfig -p | grep cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 2166
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 68839,
     "status": "ok",
     "timestamp": 1541314709952,
     "user": {
      "displayName": "Hoang Thanh",
      "photoUrl": "",
      "userId": "16863023710723783343"
     },
     "user_tz": -540
    },
    "id": "R-U6LWEQrTk1",
    "outputId": "5792c753-cc3a-4727-d880-171ce40de4d3"
   },
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/thanhhvnqb/Mxnet_colab/master/install_mxnet_cu92mkl.sh -O install_mxnet_cu92mkl.sh \n",
    "# !wget https://raw.githubusercontent.com/thanhhvnqb/Mxnet_colab/master/install_mxnet_cu90mkl.sh -O install_mxnet_cu92mkl.sh \n",
    "!bash install_mxnet_cu92mkl.sh\n",
    "# # Fix error in installing google-drive-ocamfuse\n",
    "# !wget https://launchpad.net/~alessandro-strada/+archive/ubuntu/google-drive-ocamlfuse-beta/+build/15331130/+files/google-drive-ocamlfuse_0.7.0-0ubuntu1_amd64.deb\n",
    "# !dpkg -i google-drive-ocamlfuse_0.7.0-0ubuntu1_amd64.deb\n",
    "# !apt-get install -f\n",
    "# !apt-get -y install -qq fuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 107
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 39165,
     "status": "ok",
     "timestamp": 1541314749175,
     "user": {
      "displayName": "Hoang Thanh",
      "photoUrl": "",
      "userId": "16863023710723783343"
     },
     "user_tz": -540
    },
    "id": "be8GpqQkl6xo",
    "outputId": "1cef6979-0df4-4250-c816-5376d0302088"
   },
   "outputs": [],
   "source": [
    "from google.colab import auth\n",
    "auth.authenticate_user()\n",
    "from oauth2client.client import GoogleCredentials\n",
    "creds = GoogleCredentials.get_application_default()\n",
    "import getpass\n",
    "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
    "vcode = getpass.getpass()\n",
    "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UUU78Ipr-K3P"
   },
   "outputs": [],
   "source": [
    "!mkdir -p drive\n",
    "!google-drive-ocamlfuse drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 510
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 16566,
     "status": "ok",
     "timestamp": 1541314771430,
     "user": {
      "displayName": "Hoang Thanh",
      "photoUrl": "",
      "userId": "16863023710723783343"
     },
     "user_tz": -540
    },
    "id": "gDSSsM-vxElK",
    "outputId": "e659298a-677d-458e-b309-ef3d1be7870d"
   },
   "outputs": [],
   "source": [
    "!ls drive/Working\n",
    "!ls drive/Working/cifar10\n",
    "# !mkdir -p /root/.mxnet/datasets/\n",
    "# !cp drive/Working/cifar10/cifar10.tar.gz cifar10.tar.gz\n",
    "!ls\n",
    "!tar -xzvf drive/Working/cifar10/cifar10.tar.gz \n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dqpEUD4f_WNi"
   },
   "source": [
    "# Train Cifar100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OYhqiQmVCSvv"
   },
   "source": [
    "## Initialize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "byXAC0GJCYr_"
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import sys, argparse, time, logging, random, math\n",
    "\n",
    "import numpy as np\n",
    "import mxnet as mx\n",
    "\n",
    "from mxnet import gluon, nd\n",
    "from mxnet import autograd as ag\n",
    "from mxnet.gluon import nn\n",
    "from mxnet.gluon.data.vision import transforms\n",
    "from mxboard import SummaryWriter\n",
    "\n",
    "from gluoncv.model_zoo import get_model\n",
    "from gluoncv.utils import makedirs, TrainingHistory\n",
    "from gluoncv.data import transforms as gcv_transforms\n",
    "\n",
    "from mxnet_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R-F4d6CcD39X"
   },
   "outputs": [],
   "source": [
    "################################################################\n",
    "# Data Augmentation and Data Loader\n",
    "# ---------------------------------\n",
    "#\n",
    "# Data augmentation is a common technique used for training. It is\n",
    "# base on the assumption that, for the same object, photos under different\n",
    "# composition, lighting condition, or color should all yield the same prediction.\n",
    "#\n",
    "# Here are photos of the Golden Bridge, taken by many people,\n",
    "# at different time from different angles.\n",
    "# We can easily tell that they are photos of the same thing.\n",
    "#\n",
    "# |image-golden-bridge|\n",
    "#\n",
    "# We want to teach this invariance to our model, by playing \"augmenting\"\n",
    "# input image. Our augmentation transforms the image with\n",
    "# resizing, cropping, flipping and other techniques.\n",
    "#\n",
    "# With ``Gluon``, we can create our transform function as following:\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "    # Randomly crop an area, and then resize it to be 32x32\n",
    "    gcv_transforms.RandomCrop(32, pad=4),\n",
    "    # Randomly flip the image horizontally\n",
    "    transforms.RandomFlipLeftRight(),\n",
    "    # Randomly jitter the brightness, contrast and saturation of the image\n",
    "#     transforms.RandomColorJitter(brightness=0.1, contrast=0.1, saturation=0.1),\n",
    "    # Randomly adding noise to the image\n",
    "#     transforms.RandomLighting(0.1),\n",
    "    # Transpose the image from height*width*num_channels to num_channels*height*width\n",
    "    # and map values from [0, 255] to [0,1]\n",
    "    transforms.ToTensor(),\n",
    "    # Normalize the image with mean and standard deviation calculated across all images\n",
    "    transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])\n",
    "])\n",
    "\n",
    "################################################################\n",
    "# You may have noticed that most of the operations are randomized. This in effect\n",
    "# increases the number of different images the model sees during training.\n",
    "# The more data we have, the better our model generalizes over\n",
    "# unseen images.\n",
    "#\n",
    "# On the other hand, when making prediction, we would like to remove all\n",
    "# random operations in order to get a deterministic result. The transform\n",
    "# function for prediction is:\n",
    "\n",
    "transform_test = transforms.Compose([\n",
    "#     transforms.Resize(32),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.4914, 0.4822, 0.4465], [0.2023, 0.1994, 0.2010])\n",
    "])\n",
    "\n",
    "def label_transform(label, classes):\n",
    "    ind = label.astype('int')\n",
    "    res = nd.zeros((ind.shape[0], classes), ctx = label.context)\n",
    "    res[nd.arange(ind.shape[0], ctx = label.context), ind] = 1\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9v0u_hXyEzhg"
   },
   "source": [
    "## Init Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wVRXNQQOD7ql"
   },
   "outputs": [],
   "source": [
    "################################################################\n",
    "# Note that it is important to keep the normalization step, since the\n",
    "# model only works well on inputs from the same distribution.\n",
    "#\n",
    "# With the transform functions, we can define data loaders for our\n",
    "# training and validation datasets.\n",
    "\n",
    "# Number of data loader workers\n",
    "num_workers = 8\n",
    "# Calculate effective total batch size\n",
    "batch_size = 128\n",
    "classes = 100\n",
    "# Set train=True for training data\n",
    "# Set shuffle=True to shuffle the training data\n",
    "train_data = gluon.data.DataLoader(\n",
    "    gluon.data.vision.CIFAR100(root='cifar100',train=True).transform_first(transform_train),\n",
    "    batch_size=batch_size, shuffle=True, last_batch='discard', num_workers=num_workers)\n",
    "\n",
    "# Set train=False for validation data\n",
    "val_data = gluon.data.DataLoader(\n",
    "    gluon.data.vision.CIFAR100(root='cifar100',train=False).transform_first(transform_test),\n",
    "    batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "max_step = len(train_data)\n",
    "# !ls\n",
    "# !tar -czvf cifar10.tar.gz cifar10\n",
    "# !ls\n",
    "# !cp cifar10.tar.gz drive/Working/cifar10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dYMB0Nq2FC0C"
   },
   "outputs": [],
   "source": [
    "loss_fn = gluon.loss.SoftmaxCrossEntropyLoss(sparse_label=False)\n",
    "metric = mx.metric.Accuracy()\n",
    "train_metric = mx.metric.RMSE()\n",
    "def test(ctx, val_data):\n",
    "    progbar = Progbar(target=len(val_data), prefix='Val   - ')\n",
    "    metric = mx.metric.Accuracy()\n",
    "    for i, batch in enumerate(val_data):\n",
    "        data = gluon.utils.split_and_load(batch[0], ctx_list=ctx, batch_axis=0)\n",
    "        label = gluon.utils.split_and_load(batch[1], ctx_list=ctx, batch_axis=0)\n",
    "        outputs = [net(X) for X in data]\n",
    "        metric.update(label, outputs)\n",
    "        progbar.update(i + 1)\n",
    "    return metric.get()\n",
    "\n",
    "# number of GPUs to use\n",
    "num_gpus = 1\n",
    "# ctx = [mx.gpu(i) for i in range(num_gpus)]\n",
    "ctx = [mx.gpu(0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "690OdSjOPhJW"
   },
   "source": [
    "## Setup Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fVG6fBUbjSm-"
   },
   "outputs": [],
   "source": [
    "# Setup logs and saved params directory\n",
    "net_name = 'pydmobilenet56_0.75_concat'\n",
    "work_dir = 'out/cifar100/%s/' % net_name\n",
    "saved_dir = work_dir + '/params/'\n",
    "# work_dir = 'msresnet'\n",
    "logsw = SummaryWriter(logdir=work_dir + \"/logs/\")\n",
    "get_ipython().system_raw(\n",
    "    'mkdir {}/'.format(saved_dir)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nD4JawiuI3Po"
   },
   "source": [
    "## Create Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 7022
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1567,
     "status": "ok",
     "timestamp": 1541323326305,
     "user": {
      "displayName": "Hoang Thanh",
      "photoUrl": "",
      "userId": "16863023710723783343"
     },
     "user_tz": -540
    },
    "id": "98ttSlfmyUte",
    "outputId": "d19cfeeb-43c0-4264-db3d-13fc60364158"
   },
   "outputs": [],
   "source": [
    "from mxnet.context import cpu\n",
    "from mxnet.gluon import nn, Parameter, HybridBlock\n",
    "from mxnet.gluon.contrib.nn import HybridConcurrent, Identity\n",
    "\n",
    "\n",
    "class ResBlock(HybridBlock):\n",
    "    def __init__(self, multiplier, stage, ordinal, num_features, stride, dropout, **kwargs):\n",
    "        super(ResBlock, self).__init__(prefix='stg%d/blk%d/' % (stage, ordinal), **kwargs)\n",
    "        with self.name_scope():\n",
    "            use_bias = False\n",
    "            dw_channels = np.int(num_features * multiplier)\n",
    "            self.btl_in = nn.HybridSequential(prefix='btl_in/')\n",
    "            with self.btl_in.name_scope():\n",
    "                self.btl_in.add(nn.BatchNorm())\n",
    "                self.btl_in.add(nn.Activation('relu'))\n",
    "                self.btl_in.add(nn.Conv2D(dw_channels, kernel_size=1, use_bias=use_bias))\n",
    "                if dropout:\n",
    "                    self.btl_in.add(nn.Dropout(dropout))\n",
    "                self.btl_in.add(nn.BatchNorm())\n",
    "                self.btl_in.add(nn.Activation('relu'))\n",
    "                    \n",
    "            self.conv3 = nn.HybridSequential(prefix='conv3/')\n",
    "            with self.conv3.name_scope():\n",
    "                self.conv3.add(nn.Conv2D(dw_channels, kernel_size=3, strides=stride, padding=1, groups=dw_channels, use_bias=use_bias))\n",
    "                if dropout:\n",
    "                    self.conv3.add(nn.Dropout(dropout))\n",
    "#                 self.conv3.add(nn.BatchNorm())\n",
    "                \n",
    "            self.conv5 = nn.HybridSequential(prefix='conv5/')\n",
    "            with self.conv5.name_scope():\n",
    "                self.conv5.add(nn.Conv2D(dw_channels, kernel_size=5, strides=stride, padding=2, groups=dw_channels, use_bias=use_bias))\n",
    "                if dropout:\n",
    "                    self.conv5.add(nn.Dropout(dropout))\n",
    "#                 self.conv5.add(nn.BatchNorm())\n",
    "                   \n",
    "            self.conv7 = nn.HybridSequential(prefix='conv7/')\n",
    "            with self.conv7.name_scope():\n",
    "                self.conv7.add(nn.Conv2D(dw_channels, kernel_size=7, strides=stride, padding=3, groups=dw_channels, use_bias=use_bias))\n",
    "                if dropout:\n",
    "                    self.conv7.add(nn.Dropout(dropout))\n",
    "#                 self.conv7.add(nn.BatchNorm())\n",
    "                \n",
    "            self.btl_out = nn.HybridSequential(prefix='btl_out/')\n",
    "            with self.btl_out.name_scope():\n",
    "                self.btl_out.add(nn.BatchNorm())\n",
    "                self.btl_out.add(nn.Activation('relu'))\n",
    "                self.btl_out.add(nn.Conv2D(num_features, kernel_size=1, use_bias=use_bias))\n",
    "                if dropout:\n",
    "                    self.btl_out.add(nn.Dropout(dropout))\n",
    "                self.btl_out.add(nn.BatchNorm())\n",
    "                    \n",
    "            if stride > 1:\n",
    "                self.downsample = nn.HybridSequential(prefix='')\n",
    "#                 self.downsample.add(nn.BatchNorm())\n",
    "#                 self.downsample.add(nn.Activation('relu'))\n",
    "                self.downsample.add(nn.Conv2D(num_features, kernel_size=1, strides=stride, padding=0,\n",
    "                                          use_bias=use_bias))\n",
    "                self.downsample.add(nn.BatchNorm())\n",
    "                # self.alpha = self.params.get(\"alpha\", shape=(1, 1), init = mx.init.One(), lr_mult=0)\n",
    "            else:\n",
    "                self.downsample = None\n",
    "                # self.alpha = self.params.get(\"alpha\", shape=(1, 1), init = mx.init.One())\n",
    "\n",
    "    def hybrid_forward(self, F, x):\n",
    "        \"\"\"Hybrid forward\"\"\"\n",
    "        if self.downsample:\n",
    "            residual = self.downsample(x)\n",
    "            # residual = F.broadcast_mul(*[alpha, residual])\n",
    "        else:\n",
    "            residual = x\n",
    "            # residual = F.broadcast_mul(*[alpha, x])\n",
    "        x = self.btl_in(x)\n",
    "        conv3 = self.conv3(x)\n",
    "        conv5 = self.conv5(x)\n",
    "        conv7 = self.conv7(x)\n",
    "        out = F.concat(*[conv3, conv5, conv7], dim=1) # If concatenate\n",
    "        # out = conv3 + conv5 + conv7 # If plus\n",
    "        out = self.btl_out(out)\n",
    "        out = out + residual\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(HybridBlock):\n",
    "    r\"\"\"Densenet-BC model from the\n",
    "    `\"Densely Connected Convolutional Networks\" <https://arxiv.org/pdf/1608.06993.pdf>`_ paper.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_init_features : int\n",
    "        Number of filters to learn in the first convolution layer.\n",
    "    growth_rate : int\n",
    "        Number of filters to add each layer (`k` in the paper).\n",
    "    block_config : list of int\n",
    "        List of integers for numbers of layers in each pooling block.\n",
    "    bn_size : int, default 4\n",
    "        Multiplicative factor for number of bottle neck layers.\n",
    "        (i.e. bn_size * k features in the bottleneck layer)\n",
    "    dropout : float, default 0\n",
    "        Rate of dropout after each dense layer.\n",
    "    classes : int, default 1000\n",
    "        Number of classification classes.\n",
    "    \"\"\"\n",
    "    def __init__(self, prefix, multiplier, num_init_features, cells, channels, dropout=0, classes=10, **kwargs):\n",
    "\n",
    "        super(ResNet, self).__init__(prefix=prefix, **kwargs)\n",
    "        with self.name_scope():\n",
    "            assert len(cells) == len(channels), \"Cells and channels should be same\"\n",
    "            self.conv0 = nn.HybridSequential('conv0/')\n",
    "            with self.conv0.name_scope():\n",
    "                self.conv0.add(nn.BatchNorm(scale=False, center=False))\n",
    "                self.conv0.add(nn.Conv2D(num_init_features, kernel_size=3,\n",
    "                                            strides=1, padding=1, use_bias=False))\n",
    "            # Add gated cnn cell\n",
    "            self.cells = nn.HybridSequential()\n",
    "            for stage in range(len(cells)):                \n",
    "                for cell in range(cells[stage]):\n",
    "                    stride = 2 if cell == 0 and stage > 0 else 1\n",
    "                    self.cells.add(ResBlock(multiplier, stage, cell, channels[stage], stride, dropout))\n",
    "                    \n",
    "            self.output = nn.HybridSequential('classifier/')\n",
    "            with self.output.name_scope():\n",
    "                self.output.add(nn.GlobalAvgPool2D())\n",
    "#                 self.output.add(nn.Flatten())\n",
    "                self.output.add(nn.Dense(classes))\n",
    "\n",
    "    def hybrid_forward(self, F, x):\n",
    "        x = self.conv0(x)\n",
    "        x = self.cells(x)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "\n",
    "# Specification\n",
    "msmobilenet_spec = {29: (32, [3, 3, 3], [32, 64, 128]),\n",
    "              56: (32, [6, 6, 6], [32, 64, 128])\n",
    "             } # 20: 3; 56: 6; 110: 18\n",
    "dropout = 0\n",
    "\n",
    "def get_pydmobilenet(prefix, num_layers, multiplier, classes=10, pretrained=False, ctx=cpu(),\n",
    "                 root='~/.mxnet/models', **kwargs):\n",
    "    num_init_features, cells, channels = msmobilenet_spec[num_layers]\n",
    "    net = ResNet(prefix, multiplier, num_init_features, cells, channels, dropout, classes, **kwargs)\n",
    "    if pretrained:\n",
    "        net.load_parameters(get_model_file('densenet%d'%(num_layers), root=root), ctx=ctx)\n",
    "    return net\n",
    "\n",
    "def pydmobilenet29_0_25(classes=100, **kwargs):\n",
    "    return get_pydmobilenet('pydmobilenet29_0_25/', 29, 0.25, classes, **kwargs)\n",
    "\n",
    "def pydmobilenet29_0_5(classes=100, **kwargs):\n",
    "    return get_msmobilenet('pydmobilenet29_0_5/', 29, 0.5, classes, **kwargs)\n",
    "\n",
    "def pydmobilenet29_0_75(classes=100, **kwargs):\n",
    "    return get_pydmobilenet('pydmobilenet29_0_75/', 29, 0.75, classes, **kwargs)\n",
    "\n",
    "def pydmobilenet29_1(classes=100, **kwargs):\n",
    "    return get_msmobilenet('pydmobilenet29_1/', 29, 1, classes, **kwargs)\n",
    "\n",
    "def pydmobilenet29_1_25(classes=100, **kwargs):\n",
    "    return get_pydmobilenet('pydmobilenet29_1_25/', 29, 1.25, classes, **kwargs)\n",
    "\n",
    "def pydmobilenet56_0_25(classes=100, **kwargs):\n",
    "    return get_pydmobilenet('pydmobilenet56_0_25/', 56, 0.25, classes, **kwargs)\n",
    "\n",
    "def pydmobilenet56_0_5(classes=100, **kwargs):\n",
    "    return get_pydmobilenet('pydmobilenet56_0_5/', 56, 0.5, classes, **kwargs)\n",
    "\n",
    "def pydmobilenet56_0_75(classes=100, **kwargs):\n",
    "    return get_pydmobilenet('pydmobilenet56_0_75/', 56, 0.75, classes, **kwargs)\n",
    "\n",
    "def pydmobilenet56_0_75(classes=100, **kwargs):\n",
    "    return get_pydmobilenet('pydmobilenet56_1/', 56, 1, classes, **kwargs)\n",
    "\n",
    "\n",
    "net = pydmobilenet56_1(classes)\n",
    "net.hybridize()\n",
    "\n",
    "net.initialize(mx.init.Xavier(), ctx=ctx)\n",
    "net.forward(mx.nd.ones((1,3, 32, 32), ctx=ctx[0]))\n",
    "total_param = count_param_gluon(net)\n",
    "logsw.add_graph(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EeLVtvdgFRCd"
   },
   "source": [
    "## Run train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "v3f4KdraE9xJ"
   },
   "outputs": [],
   "source": [
    "################################################################\n",
    "# Optimizer, Loss and Metric\n",
    "# --------------------------\n",
    "#\n",
    "# Optimizer improves the model during training. Here we use the popular\n",
    "# Nesterov accelerated gradient descent algorithm.\n",
    "\n",
    "# Learning rate decay factor\n",
    "lr_decay = 0.1\n",
    "# Epochs where learning rate decays\n",
    "epochs = 320\n",
    "opt_type = 1\n",
    "if opt_type == 1: # Nesterov accelerated gradient descent\n",
    "    lr_decay_epoch = [150, 225, np.inf]\n",
    "    optimizer = 'nag'  \n",
    "    optimizer_params = {'learning_rate': 0.1, 'wd': 0.0001, 'momentum': 0.9}\n",
    "elif opt_type == 2: # Adam\n",
    "    lr_decay_epoch = [150, 225, np.inf]\n",
    "    optimizer = 'adam'  \n",
    "    optimizer_params = {'learning_rate': 0.001}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1495
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 39619923,
     "status": "ok",
     "timestamp": 1540903245488,
     "user": {
      "displayName": "Hoang Thanh",
      "photoUrl": "",
      "userId": "16863023710723783343"
     },
     "user_tz": -540
    },
    "id": "PwjgcKgnFTcn",
    "outputId": "b2fc9d39-bb51-4c2f-e221-20200b50f94a",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# net = msmobilenet29_0_75(classes)\n",
    "# net.hybridize()\n",
    "print(net_name)\n",
    "print(total_param)\n",
    "\n",
    "train_history = TrainingHistory(['training-error', 'validation-error'])\n",
    "gstep = 0\n",
    "lr_decay_count = 0\n",
    "begin_epoch = 1\n",
    "if begin_epoch > 1:\n",
    "    net.load_parameters(saved_dir + '/pydmobilenet-%04d.params' % (begin_epoch - 1), ctx=ctx)\n",
    "    gstep = (begin_epoch - 1) * max_step\n",
    "# else:\n",
    "#     net.initialize(mx.init.Xavier(), ctx=ctx)\n",
    "\n",
    "# Define our trainer for net\n",
    "trainer = gluon.Trainer(net.collect_params(), optimizer, optimizer_params)\n",
    "best_acc = 0\n",
    "best_ep = -1\n",
    "\n",
    "for epoch in range(begin_epoch, epochs + 1):\n",
    "    print('[Epoch %d] ' % epoch)\n",
    "    progbar = Progbar(target=max_step, prefix='Train - ', stateful_metrics=['loss'])\n",
    "    tic = time.time()\n",
    "    train_metric.reset()\n",
    "    train_loss = 0\n",
    "    alpha = 1\n",
    "\n",
    "    # Learning rate decay\n",
    "    while epoch >= lr_decay_epoch[lr_decay_count]:\n",
    "        trainer.set_learning_rate(trainer.learning_rate*lr_decay)\n",
    "        lr_decay_count += 1\n",
    "\n",
    "    # Loop through each batch of training data\n",
    "    for i, batch in enumerate(train_data):\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "        if epoch >= epochs - 20:\n",
    "            lam = 1\n",
    "        gstep += 1\n",
    "#         !nvidia-smi\n",
    "        # Extract data and label\n",
    "        data_1 = gluon.utils.split_and_load(batch[0], ctx_list=ctx, batch_axis=0)\n",
    "        label_1 = gluon.utils.split_and_load(batch[1], ctx_list=ctx, batch_axis=0)\n",
    "\n",
    "        data = [lam*X + (1-lam)*X[::-1] for X in data_1]\n",
    "        label = []\n",
    "        for Y in label_1:\n",
    "            y1 = label_transform(Y, classes)\n",
    "            y2 = label_transform(Y[::-1], classes)\n",
    "            label.append(lam*y1 + (1-lam)*y2)\n",
    "\n",
    "        # AutoGrad\n",
    "        with ag.record():\n",
    "            output = [net(X) for X in data]\n",
    "            loss = [loss_fn(yhat, y) for yhat, y in zip(output, label)]\n",
    "\n",
    "        # Backpropagation\n",
    "        for l in loss:\n",
    "            l.backward()\n",
    "\n",
    "        # Optimize\n",
    "        trainer.step(batch_size)\n",
    "\n",
    "        # Update metrics\n",
    "        batch_loss = sum([l.sum().asscalar() for l in loss])\n",
    "        batch_loss /= (len(loss) * batch_size)\n",
    "        train_loss += batch_loss\n",
    "        output_softmax = [nd.SoftmaxActivation(out) for out in output]\n",
    "        train_metric.update(label, output_softmax)\n",
    "        logsw.add_scalar('loss_batch', {'pydmobilenet_train_loss': batch_loss}, global_step=gstep)\n",
    "        progbar.update(i + 1, [['loss', batch_loss]])\n",
    "        \n",
    "    train_loss /= max_step\n",
    "    name, train_acc = train_metric.get()\n",
    "    train_acc = 1 - train_acc\n",
    "    # Evaluate on Validation data\n",
    "    name, val_acc = test(ctx, val_data)\n",
    "\n",
    "    # Update history and print metrics\n",
    "    train_history.update([1 - train_acc, 1 - val_acc])\n",
    "    logsw.add_scalar('loss_ep', {'pydmobilenet_train_loss': train_loss}, global_step=epoch)\n",
    "    logsw.add_scalar('accuracy', {'pydmobilenet_train_acc': train_acc}, global_step=epoch)\n",
    "    logsw.add_scalar('accuracy', {'pydmobilenet_val_acc': val_acc}, global_step=epoch)\n",
    "    net.save_parameters(saved_dir + '/pydmobilenet-%04d.params' % epoch) \n",
    "    print('Sum   - train_acc: %.2f | val_acc: %.2f | Time=%ds' %\n",
    "                (train_acc * 100, val_acc * 100, time.time() - tic))\n",
    "    if val_acc > best_acc:\n",
    "        best_acc = val_acc\n",
    "        best_ep = epoch\n",
    "print(\"Validation: Best acc:\", best_acc * 100, \"at epoch:\", best_ep)\n",
    "# net.save_parameters(saved_dir + '/msmobilenet-0000.params')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 351
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1127,
     "status": "ok",
     "timestamp": 1541322041511,
     "user": {
      "displayName": "Hoang Thanh",
      "photoUrl": "",
      "userId": "16863023710723783343"
     },
     "user_tz": -540
    },
    "id": "kiutIMvnZBlm",
    "outputId": "5c5ebe3d-d1d4-497f-d0fd-bbcb9887f58b"
   },
   "outputs": [],
   "source": [
    "train_history.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dn40rkG15nV4"
   },
   "outputs": [],
   "source": [
    "net = msmobilenet56_1(100)\n",
    "net.hybridize()\n",
    "ctx = [mx.gpu(0)]\n",
    "net.initialize(mx.init.Xavier(), ctx=ctx)\n",
    "net.forward(mx.nd.ones((1,3, 32, 32), ctx=ctx[0]))\n",
    "# count_param_gluon(net)\n",
    "net_name = 'msmobilenet29_0_5_concat'\n",
    "work_dir = 'drive/Working/cifar10/%s/' % net_name\n",
    "saved_dir = work_dir + '/params/'\n",
    "epochs = 1\n",
    "best_acc = 0\n",
    "best_ep = -1\n",
    "val_accs = list()\n",
    "for epoch in range(1, epochs + 1):\n",
    "    net.load_parameters('models/msmobilenet-%04d.params' % 300, ctx=ctx)\n",
    "    name, val_acc = test( ctx, val_data)\n",
    "    (best_acc, best_ep) = (val_acc, epoch) if val_acc > best_acc else (best_acc, best_ep)\n",
    "    val_accs.append(val_acc)\n",
    "np.save('val_acc/cifar100_%s' % net_name, val_accs)\n",
    "print(\"Acc:\", best_acc, 'at epoch:',best_ep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "RMbWEWf1-DhP"
   ],
   "name": "Train_cifar10_msresnet.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
